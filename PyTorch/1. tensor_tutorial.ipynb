{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"GrlrIZ_5SQAz","executionInfo":{"status":"ok","timestamp":1720163226951,"user_tz":-330,"elapsed":4,"user":{"displayName":"Devendra Mani","userId":"18006349274153272674"}}},"outputs":[],"source":["# For tips on running notebooks in Google Colab, see\n","# https://pytorch.org/tutorials/beginner/colab\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"JLSbNA2uSQA0"},"source":["[Learn the Basics](intro.html) \\|\\|\n","[Quickstart](quickstart_tutorial.html) \\|\\| **Tensors** \\|\\| [Datasets &\n","DataLoaders](data_tutorial.html) \\|\\|\n","[Transforms](transforms_tutorial.html) \\|\\| [Build\n","Model](buildmodel_tutorial.html) \\|\\|\n","[Autograd](autogradqs_tutorial.html) \\|\\|\n","[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n","Model](saveloadrun_tutorial.html)\n","\n","Tensors\n","=======\n","\n","Tensors are a specialized data structure that are very similar to arrays\n","and matrices. In PyTorch, we use tensors to encode the inputs and\n","outputs of a model, as well as the model's parameters.\n","\n","Tensors are similar to [NumPy's](https://numpy.org/) ndarrays, except\n","that tensors can run on GPUs or other hardware accelerators. In fact,\n","tensors and NumPy arrays can often share the same underlying memory,\n","eliminating the need to copy data (see\n","`bridge-to-np-label`{.interpreted-text role=\"ref\"}). Tensors are also\n","optimized for automatic differentiation (we\\'ll see more about that\n","later in the [Autograd](autogradqs_tutorial.html) section). If you're\n","familiar with ndarrays, you'll be right at home with the Tensor API. If\n","not, follow along!\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YiEev3tySQA2","executionInfo":{"status":"ok","timestamp":1720163235294,"user_tz":-330,"elapsed":8346,"user":{"displayName":"Devendra Mani","userId":"18006349274153272674"}}},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"Rfkwxw2CSQA2"},"source":["Initializing a Tensor\n","=====================\n","\n","Tensors can be initialized in various ways. Take a look at the following\n","examples:\n","\n","**Directly from data**\n","\n","Tensors can be created directly from data. The data type is\n","automatically inferred.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"J0vCvO2YSQA3","executionInfo":{"status":"ok","timestamp":1720163277161,"user_tz":-330,"elapsed":630,"user":{"displayName":"Devendra Mani","userId":"18006349274153272674"}}},"outputs":[],"source":["data = [[1, 2],[3, 4]]\n","x_data = torch.tensor(data)"]},{"cell_type":"markdown","metadata":{"id":"ZnB_mI8aSQA3"},"source":["**From a NumPy array**\n","\n","Tensors can be created from NumPy arrays.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"d5dKrWg7SQA3","executionInfo":{"status":"ok","timestamp":1720163328469,"user_tz":-330,"elapsed":537,"user":{"displayName":"Devendra Mani","userId":"18006349274153272674"}}},"outputs":[],"source":["np_array = np.array(data)\n","x_np = torch.from_numpy(np_array)"]},{"cell_type":"markdown","metadata":{"id":"7FQpWRKmSQA3"},"source":["**From another tensor:**\n","\n","The new tensor retains the properties (shape, datatype) of the argument\n","tensor, unless explicitly overridden.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"G8dO4Qa0SQA3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720163595536,"user_tz":-330,"elapsed":589,"user":{"displayName":"Devendra Mani","userId":"18006349274153272674"}},"outputId":"0407cd83-089e-4c05-db2a-6fdb7be0058c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.0829, 0.7098],\n","        [0.6304, 0.1071]]) \n","\n"]}],"source":["x_ones = torch.ones_like(x_data) # retains the properties of x_data\n","print(f\"Ones Tensor: \\n {x_ones} \\n\")\n","\n","x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n","print(f\"Random Tensor: \\n {x_rand} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"n2x98GoASQA4"},"source":["**With random or constant values:**\n","\n","`shape` is a tuple of tensor dimensions. In the functions below, it\n","determines the dimensionality of the output tensor.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"paqc7sEHSQA4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720163560913,"user_tz":-330,"elapsed":729,"user":{"displayName":"Devendra Mani","userId":"18006349274153272674"}},"outputId":"69b5da41-f740-4eb6-af89-15ff5773958f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Random Tensor: \n"," tensor([[0.9370, 0.2247, 0.2256],\n","        [0.8337, 0.9703, 0.6599]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["shape = (2,3)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"]},{"cell_type":"markdown","metadata":{"id":"0zcBfVoASQA4"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"Y078rJ29SQA4"},"source":["Attributes of a Tensor\n","======================\n","\n","Tensor attributes describe their shape, datatype, and the device on\n","which they are stored.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"O6giDwO3SQA5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720163612360,"user_tz":-330,"elapsed":559,"user":{"displayName":"Devendra Mani","userId":"18006349274153272674"}},"outputId":"ae946655-1912-4e5e-ff65-c20d5a8172f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["tensor = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor.shape}\")\n","print(f\"Datatype of tensor: {tensor.dtype}\")\n","print(f\"Device tensor is stored on: {tensor.device}\")"]},{"cell_type":"markdown","metadata":{"id":"YaE3ZUmuSQA5"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"--VJ5VZqSQA5"},"source":["Operations on Tensors\n","=====================\n","\n","Over 100 tensor operations, including arithmetic, linear algebra, matrix\n","manipulation (transposing, indexing, slicing), sampling and more are\n","comprehensively described\n","[here](https://pytorch.org/docs/stable/torch.html).\n","\n","Each of these operations can be run on the GPU (at typically higher\n","speeds than on a CPU). If you're using Colab, allocate a GPU by going to\n","Runtime \\> Change runtime type \\> GPU.\n","\n","By default, tensors are created on the CPU. We need to explicitly move\n","tensors to the GPU using `.to` method (after checking for GPU\n","availability). Keep in mind that copying large tensors across devices\n","can be expensive in terms of time and memory!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC1EMtPcSQA5"},"outputs":[],"source":["# We move our tensor to the GPU if available\n","if torch.cuda.is_available():\n","    tensor = tensor.to(\"cuda\")"]},{"cell_type":"markdown","metadata":{"id":"W4Mw5I0oSQA5"},"source":["Try out some of the operations from the list. If you\\'re familiar with\n","the NumPy API, you\\'ll find the Tensor API a breeze to use.\n"]},{"cell_type":"markdown","metadata":{"id":"-LJ5PFgtSQA5"},"source":["**Standard numpy-like indexing and slicing:**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-cSINJISQA5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720122767715,"user_tz":-330,"elapsed":811,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"71085539-f3c0-4ad5-9be3-3e4a0df43c46"},"outputs":[{"output_type":"stream","name":"stdout","text":["First row: tensor([1., 1., 1., 1.])\n","First column: tensor([1., 1., 1., 1.])\n","Last column: tensor([1., 1., 1., 1.])\n","tensor([[1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.],\n","        [1., 0., 1., 1.]])\n"]}],"source":["tensor = torch.ones(4, 4)\n","print(f\"First row: {tensor[0]}\")\n","print(f\"First column: {tensor[:, 0]}\")\n","print(f\"Last column: {tensor[..., -1]}\")\n","tensor[:,1] = 0\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"TepRoOu8SQA6"},"source":["**Joining tensors** You can use `torch.cat` to concatenate a sequence of\n","tensors along a given dimension. See also\n","[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html),\n","another tensor joining operator that is subtly different from\n","`torch.cat`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaWBttm6SQA6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720122837336,"user_tz":-330,"elapsed":566,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"f8840a67-4f06-41e4-ceda-9ea11f878358"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n","        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"]}],"source":["t1 = torch.cat([tensor, tensor, tensor], dim=1)\n","print(t1)"]},{"cell_type":"markdown","metadata":{"id":"0wzM2tDxSQA6"},"source":["**Arithmetic operations**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWKE1o61SQA6"},"outputs":[],"source":["# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n","# ``tensor.T`` returns the transpose of a tensor\n","y1 = tensor @ tensor.T\n","y2 = tensor.matmul(tensor.T)\n","\n","y3 = torch.rand_like(y1)\n","torch.matmul(tensor, tensor.T, out=y3)\n","\n","\n","# This computes the element-wise product. z1, z2, z3 will have the same value\n","z1 = tensor * tensor\n","z2 = tensor.mul(tensor)\n","\n","z3 = torch.rand_like(tensor)\n","torch.mul(tensor, tensor, out=z3)"]},{"cell_type":"markdown","metadata":{"id":"L6Jdvv9jSQA6"},"source":["**Single-element tensors** If you have a one-element tensor, for example\n","by aggregating all values of a tensor into one value, you can convert it\n","to a Python numerical value using `item()`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYrNtqkuSQA6"},"outputs":[],"source":["agg = tensor.sum()\n","agg_item = agg.item()\n","print(agg_item, type(agg_item))"]},{"cell_type":"markdown","metadata":{"id":"CWsytPJ8SQA7"},"source":["**In-place operations** Operations that store the result into the\n","operand are called in-place. They are denoted by a `_` suffix. For\n","example: `x.copy_(y)`, `x.t_()`, will change `x`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3LdIt4lSQA7"},"outputs":[],"source":["print(f\"{tensor} \\n\")\n","tensor.add_(5)\n","print(tensor)"]},{"cell_type":"markdown","metadata":{"id":"cKWurnr1SQA7"},"source":["<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n","<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n","<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate lossof history. Hence, their use is discouraged.</p>\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"2Hfj3_CNSQA7"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"id5NquR5SQA7"},"source":["Bridge with NumPy {#bridge-to-np-label}\n","=================\n","\n","Tensors on the CPU and NumPy arrays can share their underlying memory\n","locations, and changing one will change the other.\n"]},{"cell_type":"markdown","metadata":{"id":"aEfjZj8tSQA7"},"source":["Tensor to NumPy array\n","=====================\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAAVK_LiSQA8"},"outputs":[],"source":["t = torch.ones(5)\n","print(f\"t: {t}\")\n","n = t.numpy()\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"AJYe144HSQA8"},"source":["A change in the tensor reflects in the NumPy array.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JovrsriVSQA8"},"outputs":[],"source":["t.add_(1)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]},{"cell_type":"markdown","metadata":{"id":"BobGGGAWSQA8"},"source":["NumPy array to Tensor\n","=====================\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0HcuqStUSQA8"},"outputs":[],"source":["n = np.ones(5)\n","t = torch.from_numpy(n)"]},{"cell_type":"markdown","metadata":{"id":"JGBgnDVjSQA8"},"source":["Changes in the NumPy array reflects in the tensor.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t88LerP8SQA8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720122866268,"user_tz":-330,"elapsed":478,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"5b7a7816-3e0c-4aa4-84de-2a7ade16774b"},"outputs":[{"output_type":"stream","name":"stdout","text":["t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n","n: [2. 2. 2. 2. 2.]\n"]}],"source":["np.add(n, 1, out=n)\n","print(f\"t: {t}\")\n","print(f\"n: {n}\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}