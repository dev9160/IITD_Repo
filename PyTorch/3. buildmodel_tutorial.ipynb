{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9heFdYsFYoHB"},"outputs":[],"source":["# For tips on running notebooks in Google Colab, see\n","# https://pytorch.org/tutorials/beginner/colab\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"F5ebnJbjYoHC"},"source":["[Learn the Basics](intro.html) \\|\\|\n","[Quickstart](quickstart_tutorial.html) \\|\\|\n","[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n","DataLoaders](data_tutorial.html) \\|\\|\n","[Transforms](transforms_tutorial.html) \\|\\| **Build Model** \\|\\|\n","[Autograd](autogradqs_tutorial.html) \\|\\|\n","[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n","Model](saveloadrun_tutorial.html)\n","\n","Build the Neural Network\n","========================\n","\n","Neural networks comprise of layers/modules that perform operations on\n","data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace\n","provides all the building blocks you need to build your own neural\n","network. Every module in PyTorch subclasses the\n","[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n","A neural network is a module itself that consists of other modules\n","(layers). This nested structure allows for building and managing complex\n","architectures easily.\n","\n","In the following sections, we\\'ll build a neural network to classify\n","images in the FashionMNIST dataset.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"xoef9miOYoHD","executionInfo":{"status":"ok","timestamp":1720159206936,"user_tz":-330,"elapsed":17116,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}}},"outputs":[],"source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","metadata":{"id":"XvsJhenQYoHE"},"source":["Get Device for Training\n","=======================\n","\n","We want to be able to train our model on a hardware accelerator like the\n","GPU or MPS, if available. Let\\'s check to see if\n","[torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) or\n","[torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) are\n","available, otherwise we use the CPU.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"fwFcNbbgYoHE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720159211454,"user_tz":-330,"elapsed":520,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"b58250eb-45a3-46dd-c2d1-78b62e819449"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n"]}],"source":["device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\"\n","    if torch.backends.mps.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"markdown","metadata":{"id":"YLRljeWKYoHE"},"source":["Define the Class\n","================\n","\n","We define our neural network by subclassing `nn.Module`, and initialize\n","the neural network layers in `__init__`. Every `nn.Module` subclass\n","implements the operations on input data in the `forward` method.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lmnqAfPRYoHF","executionInfo":{"status":"ok","timestamp":1720159223465,"user_tz":-330,"elapsed":478,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}}},"outputs":[],"source":["class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"yx4SKA1HYoHF"},"source":["We create an instance of `NeuralNetwork`, and move it to the `device`,\n","and print its structure.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"oNS5scHnYoHF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720159643992,"user_tz":-330,"elapsed":559,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"10f9fc62-4e4f-45af-b0ee-021542e13501"},"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["model = NeuralNetwork().to(device)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"PNqd3MEMYoHF"},"source":["To use the model, we pass it the input data. This executes the model\\'s\n","`forward`, along with some [background\n","operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866).\n","Do not call `model.forward()` directly!\n","\n","Calling the model on the input returns a 2-dimensional tensor with dim=0\n","corresponding to each output of 10 raw predicted values for each class,\n","and dim=1 corresponding to the individual values of each output. We get\n","the prediction probabilities by passing it through an instance of the\n","`nn.Softmax` module.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wE6C3kYwYoHG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720159897800,"user_tz":-330,"elapsed":667,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"3d7edbe4-1763-4b6f-d9f5-25a9e4f0a08d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class: tensor([4])\n"]}],"source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class: {y_pred}\")"]},{"cell_type":"markdown","metadata":{"id":"gC0p0TxoYoHG"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"L1ZMdcBlYoHG"},"source":["Model Layers\n","============\n","\n","Let\\'s break down the layers in the FashionMNIST model. To illustrate\n","it, we will take a sample minibatch of 3 images of size 28x28 and see\n","what happens to it as we pass it through the network.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"rMoOJ4UaYoHG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720159990793,"user_tz":-330,"elapsed":488,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"ee2ebaa0-46d8-4861-b864-339dddee9f35"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}],"source":["input_image = torch.rand(3,28,28)\n","print(input_image.size())"]},{"cell_type":"markdown","metadata":{"id":"EUdJSXnJYoHG"},"source":["nn.Flatten\n","==========\n","\n","We initialize the\n","[nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)\n","layer to convert each 2D 28x28 image into a contiguous array of 784\n","pixel values ( the minibatch dimension (at dim=0) is maintained).\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"VmaZNYWLYoHH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720159996971,"user_tz":-330,"elapsed":428,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"b0242389-6a8d-4c69-b059-1764ccdfb71e"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}],"source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"]},{"cell_type":"markdown","metadata":{"id":"H0X-SsgfYoHH"},"source":["nn.Linear\n","=========\n","\n","The [linear\n","layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","is a module that applies a linear transformation on the input using its\n","stored weights and biases.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"VvVHfu6uYoHH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720160004417,"user_tz":-330,"elapsed":1685,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"c7800156-afc5-4eb0-a178-3f459b09aaea"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}],"source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"]},{"cell_type":"markdown","metadata":{"id":"3a27Rrf6YoHH"},"source":["nn.ReLU\n","=======\n","\n","Non-linear activations are what create the complex mappings between the\n","model\\'s inputs and outputs. They are applied after linear\n","transformations to introduce *nonlinearity*, helping neural networks\n","learn a wide variety of phenomena.\n","\n","In this model, we use\n","[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n","between our linear layers, but there\\'s other activations to introduce\n","non-linearity in your model.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"EEhfv9HaYoHH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720160020420,"user_tz":-330,"elapsed":499,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"f2c2f4d9-f5e4-48e8-e5e7-4d6a6446686d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU: tensor([[ 0.2218,  0.4700,  0.1302,  0.5150, -0.1220, -0.0043,  0.0596, -0.3969,\n","          0.3452,  0.5089, -0.3534, -0.4337,  0.4211,  0.7175, -0.0433,  0.5322,\n","         -0.1707,  0.0046,  0.4520, -0.0795],\n","        [ 0.1954,  0.5550,  0.4564,  0.6271,  0.3305, -0.0850,  0.0649, -0.1901,\n","          0.3216,  0.4844,  0.1774, -0.0346, -0.0093,  0.9747, -0.1009,  0.4662,\n","          0.1202,  0.4100,  0.4670,  0.0103],\n","        [ 0.1263,  0.5058,  0.2620,  0.6377,  0.0871,  0.2976,  0.2049,  0.1328,\n","          0.3061,  0.1100, -0.0508, -0.3897,  0.0451,  1.1387,  0.0657,  0.4371,\n","          0.2159,  0.0838,  0.5290, -0.3996]], grad_fn=<AddmmBackward0>)\n","\n","\n","After ReLU: tensor([[0.2218, 0.4700, 0.1302, 0.5150, 0.0000, 0.0000, 0.0596, 0.0000, 0.3452,\n","         0.5089, 0.0000, 0.0000, 0.4211, 0.7175, 0.0000, 0.5322, 0.0000, 0.0046,\n","         0.4520, 0.0000],\n","        [0.1954, 0.5550, 0.4564, 0.6271, 0.3305, 0.0000, 0.0649, 0.0000, 0.3216,\n","         0.4844, 0.1774, 0.0000, 0.0000, 0.9747, 0.0000, 0.4662, 0.1202, 0.4100,\n","         0.4670, 0.0103],\n","        [0.1263, 0.5058, 0.2620, 0.6377, 0.0871, 0.2976, 0.2049, 0.1328, 0.3061,\n","         0.1100, 0.0000, 0.0000, 0.0451, 1.1387, 0.0657, 0.4371, 0.2159, 0.0838,\n","         0.5290, 0.0000]], grad_fn=<ReluBackward0>)\n"]}],"source":["print(f\"Before ReLU: {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU: {hidden1}\")"]},{"cell_type":"markdown","metadata":{"id":"rd8Mb3kLYoHH"},"source":["nn.Sequential\n","=============\n","\n","[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n","is an ordered container of modules. The data is passed through all the\n","modules in the same order as defined. You can use sequential containers\n","to put together a quick network like `seq_modules`.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"ScrsnZm_YoHI","executionInfo":{"status":"ok","timestamp":1720160029959,"user_tz":-330,"elapsed":3,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}}},"outputs":[],"source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20, 10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"]},{"cell_type":"markdown","metadata":{"id":"Fjh6lTIFYoHI"},"source":["nn.Softmax\n","==========\n","\n","The last linear layer of the neural network returns [logits]{.title-ref}\n","- raw values in \\[-infty, infty\\] - which are passed to the\n","[nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)\n","module. The logits are scaled to values \\[0, 1\\] representing the\n","model\\'s predicted probabilities for each class. `dim` parameter\n","indicates the dimension along which the values must sum to 1.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xNw39uH7YoHI","executionInfo":{"status":"ok","timestamp":1720160035852,"user_tz":-330,"elapsed":714,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}}},"outputs":[],"source":["softmax = nn.Softmax(dim=1)\n","pred_probab = softmax(logits)"]},{"cell_type":"markdown","metadata":{"id":"4Ouw24ihYoHI"},"source":["Model Parameters\n","================\n","\n","Many layers inside a neural network are *parameterized*, i.e. have\n","associated weights and biases that are optimized during training.\n","Subclassing `nn.Module` automatically tracks all fields defined inside\n","your model object, and makes all parameters accessible using your\n","model\\'s `parameters()` or `named_parameters()` methods.\n","\n","In this example, we iterate over each parameter, and print its size and\n","a preview of its values.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Tak-oFI3YoHI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720160045472,"user_tz":-330,"elapsed":731,"user":{"displayName":"Richa thakur","userId":"03985798252246170929"}},"outputId":"cadf6a4b-d39f-408a-a7f6-c2718f3b3f06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure: NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","\n","\n","Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0138, -0.0036,  0.0223,  ...,  0.0067,  0.0296, -0.0343],\n","        [-0.0267,  0.0319,  0.0357,  ...,  0.0296,  0.0005, -0.0092]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0076, 0.0108], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0007, -0.0283, -0.0411,  ...,  0.0237, -0.0115,  0.0218],\n","        [ 0.0330,  0.0346, -0.0185,  ...,  0.0183,  0.0119, -0.0048]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0072, -0.0335], grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0219,  0.0064, -0.0027,  ...,  0.0441,  0.0286, -0.0046],\n","        [ 0.0181,  0.0398, -0.0260,  ...,  0.0144,  0.0428, -0.0072]],\n","       grad_fn=<SliceBackward0>) \n","\n","Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0051, -0.0069], grad_fn=<SliceBackward0>) \n","\n"]}],"source":["print(f\"Model structure: {model}\\n\\n\")\n","\n","for name, param in model.named_parameters():\n","    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"H39lqLZkYoHI"},"source":["------------------------------------------------------------------------\n"]},{"cell_type":"markdown","metadata":{"id":"-YiJPNsXYoHJ"},"source":["Further Reading\n","===============\n","\n","-   [torch.nn API](https://pytorch.org/docs/stable/nn.html)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}